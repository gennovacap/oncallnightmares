
Speaker 1: you're listening to the on call nightmares podcast in every week, I bring you conversations with technologists who spent some time on call this week. I come to you from Stockholm, Sweden, where I am taking part in the Microsoft Ignite the tour thing that we're doing. I guess we'll just call it a thing, and it's been really great. I got to meet a lot of people. I got to wear a bath restage. Well, I didn't bath three T shirt. Why did my talk today? And that in itself was really great. So obviously I'm not in a normal space, so sound quality isn't quite as great as it normally is, so I do apologize for that. But let's let's get on with the normal things that I do, which is one. If you'd like to be on the podcast, it's extremely easy. You can send me an email. It's article nightmares at gmail dot com, or reach out to me on Twitter. It's at on call nightmares or I stay at on nickel night, Mayor, no less. Sorry, Twitter doesn't give me enough characters for that s or you could just reach out to me personally at J destro eso This week I spoke to the person who gets woken up for you to get woken up, and that would be a Roop. Chaka Party off off pager duty Group has been working the Spaces software since about 2007. Guy went to Harvard Medical School and then made a decision. Thio become involved in systems and software development operations, and that alone is really amazing. Just hearing his insight into things was pretty cool. So let's move on to the podcast for this week. Let's hear with our conversation with rope and hopefully you enjoy it and you learn as much as I did. So let's get into it. Okay? So once again, you are listening to the on call nightmares podcast, and every week I bring you conversations with technologists who spent time on call. So I know I have brought along some heavy hitters in the past, and this time I wanted to go, um, almost conception like I wanted to find out who wakes up the people who wake us up, and by doing that, I've been lucky enough to have a conversation say, with a root chakra party of pager duty He is the director of engineering. Um, he's been doing a lot since what, around 2007? Yeah. You're talking about operations, engineering, Amazon. Um, you helped with the Amazon marketplace and actually leading teams as well. Loza teams over at Netflix to improve availability and reliability. But right now you're helping with infrastructure as a director of engineering over at pager duty. So hello, Ruth. Thank you very much for your time today.

Speaker 2: I'll thank you for having me j appreciate

Speaker 1: it. Awesome. So I'm gonna let you take a quick victory lap on then I want to get into finding out more about you, but I wanted to congratulate pager duty. All just went public. What? A week ago. And, um, I can say you've got a product that I know I've been It's been a part of my career, and a lot of other people probably listen this by caste career. Was it really cool to just finally crossed that line?

Speaker 2: Yeah. You know, it was you're one of the things about going public that you never see your jen stock, but it's it's kind of like a marriage and that Yes, it's a big, exciting day and you prepped for for for weeks, months and years. And then it happens. And then the hard part's actually staying married. And so now that another pass it. It was a fantastic celebration. It's a great recognition from not just, you know, the public markets itself, but really, you know, the only way we got here was actually prep petitioners like yourself and the rest of our customers said, We're incredibly honored and humbled by by this kind of recognition. But for us, this is just another milestone, and we got to keep working harder and harder to make our customers happy. Er,

Speaker 1: yeah, I think that you all reaching that public level says to everyone that actually, the work that they're doing by utilizing your product is also successful. People who are putting together rotations for whether it's I t teams or people who are part of other non I t. Based like I've heard of where people use pager duty for things like reminding their kids to have diabetes are levels whether or not they should take more insulin. Like I heard about this and I just think it's really amazing on you got some really great people there. My friend Matt is ah, great part of that team. So, um, congratulations. But, uh, now, uh, it's time to start talking about the really cool stuff. I think aside from going public, and that's being on call. So you've been a technologist for awhile. Let me hear a little bit about how you got started being in on call Resource is in in your career, I suppose began. So you know what

Speaker 2: the you know, all kind of do a quick replay off my career. So I actually started out before it was officially in the soft industry. I was actually doing a lot of medical research. So when I was during my undergraduate work, I studied bioengineering, and I actually have plans of medical school. Of course. What do doctors do? They got one call, you know, the end that was just part of their lifestyle. And so when I was working in research, there was actually times where you way, we're doing research on on things that could have risen one on. And you have a very short time span when you have the right kind of subject, the right kind of testing everything So So we actually have the notion of uncle research law that I was working in a pate. You know, patients come in with a very specific case that, you know, matches the research that Rich do. They're willing to be part of it. And so, yeah, there were times where it didn't matter wherever I was. If I could please that Saturday night, that was my Saturday night and I had to physically drive in to the hospital and thes things. But But then, even later, as I entered the softer industry, a lot of that that kind of preparing is translated very cleanly into how we did in stimulatory. It's responsible previous company. So, you know, like he said, I worked at Amazon for about four years and change. And there, you know, I had the opportunity to work with just some phenomenal engineering teams, some phenomenal in two years. And, you know, initially my role was acting as as an operation support engineer, working with a lot of other engineer routines and helping them figure out like, Hey, what is the best way to go? Call had to be monitoring Inspector service is when something does break me automates from the fixes that can you preferably do instead of getting people woken up. And so you know, the Amazon I learned a lot more about What does it look like for you? Not just individuals to go on call, but for teams, departments, companies to come on call, by the time I had joining them is that they had gone through what I'd consider, like their Big Dev ops transition and so there towards kind of the tail end of it. And it was really interesting to see, like, Wow, this actually does work at scale between the The company had figured out a lot of interesting ways to How do you orchestrate a responsible 100 people at once effectively? And you know, then I had the opportunity Thio then work at Netflix again with just phenomenal engineers, Mama mule teams and your Netflix was there. This was an interesting year transition that I was there where, you know, the company was heavily investing warm or streaming platform. You know, we forget sometimes, but not let's used to ship DVDs in that. And as the yes CEO naff, let's re Hastings would say that they were trying to get the legacy of of video delivery down from two days toe $200 seconds on and said that was you know what they did.

Speaker 1: It was a huge. It was a huge kind of shift on bit's and nothing to rocks. But what I really appreciate about the time that you're at Amazon and then over into Netflix is the birth of eight of us starting to happen around now. Absolutely. That's really when the cloud as an idea beyond just a simple storage becomes something where you start seeing compute jobs, go into the cloud, and then eventually it becomes something that's part of everyday business for people. So I know Netflix was one of the very first all in it, 100%. All in on aws. Um, were you part of, like some of those early, um, like building reliability with tools over? Damn it. Uh, you learn from Amazon over

Speaker 2: at Netflix? Yeah. So one of the team that I have the privilege of magic was the cloud operations of reliability engineering team that on Netflix. And there's a family Gregers l but the treatments are working. So he is the the father of Chaos Monkey. And so I had the chance to work indirectly in the team that worked on the engineers on the team built out some other members of U boats called Simeon armies of things like chaos, gorilla and chaos calm The engineers. Cory Bertram is now on the tee piece of infrastructure. Did all he had been? He had built out a lot of that tooling and the team sense, You know, I was. I love Netflix after about a year, but the team has grown quite a bit and they publish books. Something's chaos, engineering all this amazing, amazing work. And so while

Speaker 1: we don't yeah, just between Whitney Nora Jones had done, I've seen her do some of really great talks when she was with Netflix and what Colton from Gremlin is going on. Two. DOAs. Well, it's been really cool to see how an entire, like practice of technology came out of, Ah, very, very small portion of an engineering department. Yeah, and that's amazing to me.

Speaker 2: What's what's so interesting about that? Tears like if you're going back like you know, 10 ish years again, as companies are, you think about the cloud. It was the scary thing, right? You were terrified of it. And And athletes have that pretty bold decision of like, no, we're gonna let eight of us Bill data centers, not us. And but what they didn't realize was wait, we can't just forklift all of our processes all over deployment tools, everything that we did in our data center to have us. We have team

Speaker 1: no one. No one,

Speaker 2: exactly. You you can't. You can't just, you know, lift and ship these things. And that's where you know, when things like chaos Kass monkey was released. It was to really help engineers understand the cake. This is a different way of working now, In fact, we're gonna act after the practice, this stuff being production, because guess what? That's actually what reality is now. And you know the cool thing that that that I saw the Netflix was people really understood the importance of it because one I think they appreciated, instead of getting woken up at three. In the morning for Amazon decided builder in It was at two. In the afternoon and it was within their own control. Like this was something that that they don't have toe get interrupted on off hours to practice. And then the other piece was like they saw themselves. How you again, early eight of US technologies. Early cloud signals were unstable. They had a lot of challenges. You had what I loved about the necklace philosophy, which was Look, the end the day the end customer doesn't give a crap about whether the instance died or wife or crew was responsible. All they Yeah, they want to do something and they could it. And as, ah, service provider, you have the responsibility to insulate your customers from this

Speaker 1: room. You have the whole idea of 99.999% is okay because you're never going to ever really notice that. But the point that you're getting that 100 percentile that you you're gonna miss, it's so rare. And so I get you know, I've heard a lot of people talk about this is the way that you should look at monitoring in general nowadays is that 100% of time is no longer feasible because everything's always breaking, and you should make the assumption that your solution is not at 100% at all times, and you should have the tooling in the wherewithal in your automation to know that's something. When it dies, it's replaced and you can move on with their lives. And I think that's important for what we look at his modern operations. But you spent time over Netflix, then you spend a little bit of time heavy bit and you program advisory on. Then you come over the pager duty so that that's a good probably taking some changes in what you were doing. But when you're back at pager duty, um, now you're working as an engineering manager and you're part of this really growing product that I'm a magic You're throwing a lot of hardware or I should say, instances at and things like that to tell me, um, a really ugly story from that time. But while you tell me that story, we gotta go over the rules first. That's the way this podcast parts. So don't incriminate yourself because we don't need you to get in trouble. You work there now, you know, don't incriminate others because, as we say in the world of Dev Ops s sorry, blameless nous is the way we do retrospectives and help us learn. Because this podcast is a learning experience for me, For everyone listening. So you're building out pager duty and you yourself a wrong call. I would imagine at some point it's still on. So talk to me about a nasty incident and how it impacted you. And also maybe how it impacted the business agency.

Speaker 2: Yeah, I'll share one that, you know, we've actually talked a little bit about publicly, so I don't know my subject. Lee, if I'm in trouble, I'm already in trouble of these. They're so let's go. Enough. Um, this was Ah, this was a couple years back, but, uh, we have this really interesting bug with our NTP clusters. So like many companies, we have data stores that rely on and t. P. So we have to have some pretty resilient NTV network time protocol. We have to make sure our servers are on the same time. And, uh, you know, we use a variety of database technologies and one of them being Cassandra's, which again is one of the way Cassandra decides. Like, you know which right here Denver versus which Reed winds is based on time. So We have pretty pretty sharp tolerances that we wanted maintained with our DP clusters. And it's This was this Michael three years ago and one of my teams that are safe, reliability, integrity because this nd piece of shared service they actually run set of servers that access the pool for all over structure. So all of a sudden we start seeing like this is important. Of course it's on a Friday. It's always

Speaker 1: it's always always D N S A. It's always on Friday one,

Speaker 2: actually. And so what's what's happening is, you know, we have the the SA 18 starts getting some alerts based on, you know, and tp acting a little bit funny. It's, uh, they get alerted for it. And sure enough, now almost every other engineering team we have the company is also getting alerted for it because just weird things happened when time starts to drift on your servers and and so the SA 18 you jump on the jump on it pretty quickly in the area is like, Wait a second. This is This is pretty bad, like we don't know exactly what it is yet, but let's proactively get ahold of Hunt now, of course, because Paige, be all of our engineering teams are on call. And because from data stores as well, they're already pay. So by the time he spun up a major incident response internally, everyone was already up Ready

Speaker 1: Drinking alerts about database consistency. You're getting alerts about the alerts. You're getting alerts Book that storage itself. Things serum scary.

Speaker 2: Yeah. So this is where, like, basically, every engineering team is getting alerted for this and trying to figure out what the hell is going on here. And thankfully, you know, the S three jumps on the calls like Hey, like folks like, you know, everyone just got woken up. Everyone is She's probably panicking thinking the whole thing is burning down. But thankfully, you know, the the Aesir Bianca was able to pinpoint it very quickly, and they know it's it's due to NTP, but are wrong books not working right now. You know, basically, we had a set of procedures that we would run and it simply wasn't working. And so you basically we have, you know, there's a couple of tough decisions you make. There were in a good way. You know what the area focus needs to be. And so like from a good standpoint, it's a bit of a relief where we're able to quickly realized like, Okay, the customer impact is actually very, very small. It's not like we have another throwing five hundred's for older customers, but, you know, we know if we let it go on for too long, we might see some data consistency issues you might see some issues with, you know, ordering of events for our customers might get out of whack. And so that's where we know it's something that yeah, we can tolerate a little bit here and there, but, um, but then realized that you can't do it. So the, uh, the answer a team they end up working the issue for for a while. And, you know, unfortunately, we we had to make a tough decision when we told the engineering teams. Look, you can't turn off, you're alert. Serve. You think for a couple hours because if there's something more severe that happens in the meantime, reaction need to know about it, because that might actually change your focus. So, for example, one of our systems all of a sudden starts to get really impacted by, we might have to make the tough decision to say no. Change the anti people. They focused on a different pool instead and siphon off that service onto its difficult Or means we might need to take more drastic measures and say, You know what? Reroll the entire TP fleet? Yeah, it's gonna be painful for 30 seconds, but as long as it drives that that that consistency that we need afterwards greatly it's the short term pain for long term benefit. And so are the S R T team, along with a handful of engineers who were concentrating, woken up, coming in and out of the call all night. They're working. The issue working is you're trying a couple of different things. And what ended up fixing the issue was actually basically there was some cash that we had on the NTP servers that we had never even tried this before. It's re treatment to try wiping this so on staging service week. We did it. We liked like this cash, and it seemed to fix the issue and staging, and we basically that tough decision. Look, this is pretty risky because we don't actually know what the full impact is going to be in production. So we started small. We started with one of the with one of servers. Seemed to work, too. Okay, went through the whole pool. And then the following morning it was fixed. You know, the big learning I have from from that was incident response is that it's a team sport. I think there's there's this weird like, I think, myth that a lot of companies have were like, You have someone parked in the basement are wherever somewhere off on their own, fixing things furiously by themselves And, you know, years of being on call. And I've had to be that person.

Speaker 1: Yeah, e. I always like to call it the brunt of the fetus, Jack. Yeah, like there's always the Brent in many companies, and I only say Brent, because that's the character's name. But it's a unfortunate that, you know, there's been a lot of places where on call drifts through one person.

Speaker 2: Yeah, and it's and you know there's something that I'm just so proud of. You know, our engineers, that pager duty, you know, it's not that we take me to take these things seriously again, especially now that it potentially its customer impact. But if, uh if you ever listen to one of her calls collectively here, quite a bit of laughter, you'll actually you're along there cracking jokes. And again, not because we don't take this upstairs. Of course we d'oh! But when you've been up all night and you're exhausted and you're trying to figure something out and when you're about to swap in with someone else the one of the best ways to maintain morale and honestly my focus is to keep the mood like is to make sure that you don't take yourself too seriously again. Take the work seriously. Take the customer impact seriously. Don't take yourself seriously and make sure you're building that again. That team mindset, because that's what ultimately led to the resolution, was basically I don't actually know what I don't know if it actually came from Rs. Retake the idea of like, Hey, let's try Let's try this. That and pretty sure I came from another team and when you think about that kind of creative problem solving in the stressful environments, the last thing you want to do is to come into the environment say like, Hey, if you don't fix this within 10 minutes like you're all fired, that's the worst that he you could do for her creative problems.

Speaker 1: And I think the way that you kind of describe how like that's come together now is that we've We've taken that idea that we heard about of Dev ops years ago, and it sounds like you you've got to put into practice, and that's exactly the way it's supposed to go. That multiple teams, when you know the shit hits the fan are supposed to collaborate because of information sharing. And, um, and shared responsibility of service is in taking responsibilities for the things that you build. And it's it's really great to hear that that put into practice was successful for your evening and in what was a very difficult period.

Speaker 2: Yeah, and look, it doesn't it doesn't comfort free right? I think for a lot of companies like, you know, instead of, uh, time to resolve these talks with a time to finger pointing, right? I'm t o you know, how quickly can I get the blame off of my team? Yeah. Look, I understand why people, especially in any management positions. Think that right? Because you know, what's the usual force that most companies it's something breaks. Yeah, I get it gets fixed and then you have some person and you higher up that gets really upset and customers again, right? These are upset and you know the manager of the team, or maybe in the taxi of the team gets dragged out in front of a bunch of folks and the basic the equivalent tomatoes thrown up. Then for 45 minutes, that's what's called a postmortem meeting on. And it just it reads this like very my pian dysfunctional relationship between teams and Pete

Speaker 1: Credit driven management. To me when I say credit, I mean, like, credits, you're returning customers and decisions that are made they're after isn't a great wayto to deal with people. If you just had to give, you know, say, a $20,000 credit for customer based on something that you was a business we're not able to do, I don't believe that it's really an immediate It's, you know, you want Adam s L. A. Based on something who knows, and it costs money, but I don't see that blaming people individually and taking them in front of the rest of the business and trying to say, You know, we spend all this money and it's because y'all you know, I I just don't find value in that. And I think the more I hear about blameless nous in the way that people handle postmortems it. It sounds more like a a way to keep a business moving forward, as opposed to you eventually creating a schism based on just the fact something went wrong and you lost a little money.

Speaker 2: Yeah, and yeah, I absolutely agree with that. Anything. The other interesting thing that point out there when you can have, like, a truly blameless postmortem. You create this really wonderful sense of self accountability where all of the sudden, you know, people will want to do better. Because if you connect, say, like, you know, my my fever postmortems are the ones where yeah, we acknowledge it was a very complicated incident. Here is some of the contributing factors that led up to it. Unfortunately, here is the customer impact, and what I found is when, like you focus on those pieces more than like people like you know, room push this button and screwed everything up at this time. If you If you take the focus off of that and focus more again on this contributing factors and the customer impact I've known engineers are able to rally around like, Well, here's what we can do to mitigate this issue in the future. And it becomes more Belic systems and the processes that need to

Speaker 1: get its the collaboration. In fact, yes, a positive environment that's it's grown by management's management teams can eventually lead to a more positive, you know, environment that you can find that toil and start reducing it. Because of major incidents, you'll have an incident and then bye bye, positively sharing the failure, if you will. You can give people ideas to, to prepare and to prevent and big things better. And I think that's the collaborative environment that many businesses air shooting for.

Speaker 2: Yeah, it's hard because you know, I have the I have the lock that like you a meditation. Since we're probably in 15 20 people and to our founder's credit and honest eastward, leaderships credit the company. They've kind of really only the incorrigible, that force that kind of collaboration, and part of that is again like we have, that that develops DNA woven into the fabric of our company. Just like the nature of who our customers are, who are early employees, our leadership and our and And I totally understand that when you let's say, another company that doesn't have that, it's so hard to read that in there and to change the behaviors of the culture where people are collaborating on these very stressful situation.

Speaker 1: Yes, so that leads me to want to ask you some things to help us wrap up. One of the things I really wanted to ask you is if, um, you was a manager of people. You have your direct reports and you're also on call you as a manager who's on call. What advice would you give some other managers that are currently, um, you know, have teams of engineers their own call? How you What advice did you give to them on handling that team?

Speaker 2: Yeah, I think I'll start kind of high level, make more specific and like, tactics and something. I think that the number one thing is to just have a lot of empathy and understand that you know Uncle is hard. It is something that, yes, it is increasing heard of the job. But that doesn't mean that we should, like, expect people to go on, call for weeks, months at a time and accept that that's okay to interrupt their personalizing, interrupt every single facet. What, Yes, again, I'm a huge advocate of engineers going on call and owning their service is. But I'm not an advocate for people staying up 24 7 for 30 days straight. That's that's that's terrifying. That's that's terrible. And so, like the number one thing I tell my own interests have empathy for the people on your team, but also for yourself. Like in an interesting way. I've seen a lot of managers again with the best of intent, trying to shield the team from a lot of works that will take extra uncle ships off their engineer. Sometimes I get great intent, but that's not being empathetic to yourself. That's you know. Now you're just gonna burn yourself out instead of your your engineers, which is which is no good either. Again another another high level thing is, and I'll get more specifics that is really understanding that sure, it's around on call and understanding, like you know, what is he the reasonable load? And that's different, depending on your team. If you have a team, let's say off like 15 people that's able to do a 24 7 fall of the Sun distributed across the entire world. The on call load for that team is very different from a team of five people located a single times of that on the steps. Service is you have to they with numbers, and I find honesty. Just talking to your engineers and asking indicate what's a what's a reasonable load for Uncle. Let's discuss that can kind of help you get there. But if you can't come up with something, my, my like, you know, Rule of thumb is if the team's getting paged with five times a week, something's funny. It was wrong, but something funny is going on that you want to spend some time one and so more tactically. I think weekly on call reviews are critical that that meekly cadence, whether it's Monday afternoon, Friday after I don't care, but just that really cadence where the engineer brings up like Hey, here's the number of times I got paged last week, and here is the, you know, four or five incidents that I had to cover, and they covered the whole team. It's not just like the next uncle. That's that's really how you can enforce this culture of like giving a crap about on call health and making sure that your systems are stable and everything because, you know, the other thing I always bring up to a lot of managers is when your when your engineering gets pace like that's bad. You got that in itself. If you just often is solely for that, that would be okay. But we also remember what an engineer gets paste. That means they're lost some customers. There was something that the customer again click. The Bundy got a 500. They went to add an item to cart and couldn't do that. And now the business suffered as a result, and a really nice way the interest of let your customers and your engineers and the manager and now all of a sudden a line. Because you're reviewing those metrics, you're trying to reduce the number of instant you're trying to fix those problems as best you can, make sure that you're doing that weekly review. And then

Speaker 1: you have, like, a Prometheus for your people, if you will. Yeah, there's no other way to kind of put it, but, like create some sort of way to monitor and have metrics for the people that are doing the on call work and then determine when you're Max are your past certain thresholds that you find acceptable.

Speaker 2: Yeah, one of the things that the U. S. Sorry, principal, especially from Google, that I really love this idea. Error. Right? But you don't you don't set. Uh, that's a little of 100%. You said it first, let's say 99.95 is you a couple of minutes a year to do some maintenance? Or let's say you might have burned through it in January, so you have to be a little bit more careful with the next time you make a change. I kind of like the idea of a similar idea for for your people as well. We're like, you have a goal that you're trying to hit again. I can tell you right now the right goal for? For for a number of times, an engineer gets woken up. It's not zero, but I wish it could be. But I could tell you right now you're gonna have problems in your system where at least once a week, you know someone is gonna get woken up. Preferably they just get in Europe to during the day. But the right answers never zero. And so, with that, acknowledging that like, yeah, there's gonna be times where you are gonna have to take some additional call load but realizing that that should always be temporary, that should only be something that when it does surface up again, ideally during that weekly review, and it's hopefully something that could fix that fallen weak or health in the same day that sets the team up, for I think it's a sustainable on call work life balance not just for that week, but you know, for weeks and months to come. If you're able to get down that decade still

Speaker 1: got you so definitely review what people were doing. Keeping an eye on the numbers. Make sure that you, as the manager, are following up, hung these things and adding or removing resource is when they're needed Based on what load is kind of almost the way we look at how we look at service is so yeah, s o You gave me one little, like piece of advice earlier for people who are on call where you said keep it light and it's it to tack on to keep it light. How have you kept it? Light over the years is already one little thing that you can say. I I turned to this when when the chips were down. S

Speaker 2: oh, yeah, I guess so. One. Ah, one thing. A lot of 19 is always make fun of me about, You know, I tend to be a pretty happy, optimistic person, and so for me, it's it's actually quite hard to get me down. And I will always find the silver lining anything, even in the middle of an incident, you know, I'll be able to find something where it's like, Well, you know, at least the moon didn't hit the earth today like, you know, at least something more catastrophic. Didn't,

Speaker 1: uh, you know, at least bano seen real exactly, exactly of e you

Speaker 2: know, other tips for keeping it light, I think having having that discipline and that trust with people again, take the problem seriously. Take the incident and your process is seriously. But don't take yourself seriously. And I think from a leadership standpoint, if you're not ableto crack a joke here and there and then I'm serious, like in the middle of like there's a lull in your incident call as like you know, your we're waiting on something there we star. If you can crack a joke there, guess what your your engineers and you're the other kids on your organization. They're not

Speaker 1: going to go dress. It's

Speaker 2: just they're not gonna feel feel the knows that that that safe space tow say those things. I think the other thing way to keep it light is really focusing on that culture of warning and underst standing that, yes, you are going to make mistakes, but the idea is that you don't make the same mistake twice. That's like as a manager. That's what I get upset, right? It's not one of my engineers. One my man. Trust me, it's a mistake, but we're gonna make mistakes and and we try to minimize the impact of course, but I got upset when people don't learn from their own mistakes and really collaborative environment, when you start to see is you start to learn other mistakes. And that's when, like the real when you talk about accelerated learning roof like that's a fun place to be where all of the sudden you're sharing your post mortems in terms. So, like, yeah, Paige ready? We re actually broadcast very internally. We're very open and transparent, where anyone can listen in on our incident call taken all along with our scribe and everything. If someone for sales is curious like what's going on, he couldn't see everything. And we do that on purpose. And, you know, while I very occasionally had to ask people very politely to jump off off off the collar that they're being disrupted, it doesn't happen very often. And like I think that's another way to encourage that kind of collaborative morning environment is toe just. You make it easy for people that if they're interested, they can follow along sets of expectations that hate can be disruptive like that people do their jobs, but and then after that, you know, once the post mortem gets you created and filed away. Bielsa's send that back out to everyone. So people who were thinking like, Oh, I wonder what happened from that incident two weeks ago? Oh, here's the past morning and I am box now. So again, it's what? That intent of really over communicating, being transparent with you with some of our bodies.

Speaker 1: Great. So I wouldn't going thio take us to the end of our interview. Thank you so much. Or if this was a really great conversation and gave us a little bit of insight into the world of who wakes up the people who get woken up, But hopefully not too much because, you know, as you said, it only should have him once in a while. Shouldn't have too much. Um,

Speaker 2: you take our responsibilities and duties very seriously at the company.

Speaker 1: Absolutely. So, um, a rude shock on Twitter is how people could find you. That is a r u p c h a k. Are there any other projects or anything else you'd like to share before we kind

Speaker 2: of say good bye? Uh, you know the last thing I'll kind of leave folks with his You know, try your best to make all call the less scary for your engineers. That's something that you know against different for every company, for every team. That for every manager. Um, and that's something I see for a lot of our customers that they struggle with where it's scary initially, but again looks like yourself J. And it looks like myself. You've been doing it for a long time, so it's less scary for us. And, um, you know, as much as we can for each other within the community, let's figure out howto make it less scary.

Speaker 1: That's why I'm capturing these stories because I want to make sure that for the person on, you know, I had someone recently told me, You know, I'm just on call. I just got a call recently and I found your podcast, and it's really hoping just because I'm hearing stories of what other people are doing. And I really appreciate that because as much as I like also hearing from the other 15 20 year vet, who's like, Oh, wow, I really identified with that story. It's the person that hasn't been in those situations that's gonna learn from you and people like charity who've been on and others who have good good information on how to make this work feel a little better. Come together. Look. So, like I said, thank you very much for your time today. You can, Like I said, find a roof on Twitter. It's a rude shock. Um, thank you very much. We'll talk to you. Hopefully someday soon. Thank you for

Speaker 2: your time. Absolutely. Gem is absolute. My pleasure.

Speaker 1: Really appreciate it. I will be right back and wrap up the rest of podcast. Thank you very much. A route That was a great conversation we're able to have. And I really do appreciate your time. Pager duty has been one of those really, really important tools for so many people. And, uh, what his team seems to be doing is just tremendous. So I want to thank him a lot for his time on. Everybody overpaid your duty, including my buddy Matt. You're all doing great work. Thank you so much for helping us. But no thanks for the wake up calls. My wife is still disappointed with all of you. But that's not your fault. Anyhow, I I want to say thanks a lot for listening. It's been a lot over 20 something episodes now, and I'm really proud that we've been able to kind of have these conversations with together me and you are friends that we've had on the podcast. So, um, you know, if you would like to be on, please reach out. I'd really love to talk with you. So we'll see you next time as I have another conversation with a technologist who spent some time on call. See you later.
